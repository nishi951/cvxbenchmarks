{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with L1 Regularization\n",
    "\n",
    "In a logistic regression, we assume that the probability $p$ of a Bernoulli random variable is given by the logistic function:\n",
    "\n",
    "\\begin{equation}\n",
    "p = \\frac{\\exp(a^Tx)}{1 + \\exp(a^T x)}\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ is some parameter vector to be estimated, and $a$ is the data vector. In general, we would want to consider $a^Tx+ v$ instead of just $a^T x$, but here we assume our data set has zero mean (i.e. $E[a] = 0$) in which case it is valid to leave out the bias term $v$ for simplicity (this essentially fixes $p | a = 0$ to be $1/2$).\n",
    "\n",
    "Given data $a_1,..., a_m$ and the corresponding class labels $b_1,..., b_m \\in \\{-1, 1\\}$, The maximum likelihood estimate of $p$ is then obtained by maximizing the log-likelihood of the data:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{maximize} && \\sum_{i | b_i = 1} \\log \\frac{1}{1 + \\exp(-a^T x_i)} \n",
    "                    +\\sum_{i | b_i = -1} \\log \\frac{1}{1 + \\exp(a^T x_i)} \n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Noting that the only difference between the first and second sums is the sign of the coefficient of $a^Tx$, we can express this compactly as\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\sum_{i=1}^m \\log(1 +\\exp(-b_ia^T x_i)) \n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that we have flipped the leading sign and changed the maximize to a minimize. Since Log-sum-exp is convex, this is a convex optimization problem.\n",
    "\n",
    "If we have reason to believe that $x$ is sparse (i.e. there are relatively few important parameters in the data vector $u$) then we can add an $\\ell_1$ regularization term to the objective to obtain\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\sum_{i=1}^m \\log(1 +\\exp(-b_ia^T x_i)) + \\lambda \\|x\\|_1\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\lambda$ is a hyperparameter that controls the strength of our regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('status:', 'optimal')\n",
      "('optimal value:', 961.009369017684)\n",
      "('true optimal value:', None)\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "def normalized_data_matrix(m, n, mu):\n",
    "    if mu == 1:\n",
    "        # dense\n",
    "        A = np.random.randn(m, n)\n",
    "        A /= np.sqrt(np.sum(A**2, 0))\n",
    "    else:\n",
    "        # sparse\n",
    "        A = sps.rand(m, n, mu)\n",
    "        A.data = np.random.randn(A.nnz)\n",
    "        N = A.copy()\n",
    "        N.data = N.data**2\n",
    "        A = A*sps.diags([1 / np.sqrt(np.ravel(N.sum(axis=0)))], [0])\n",
    "\n",
    "    return A\n",
    "\n",
    "def create_classification(m, n, rho=1, mu=1, sigma=0.05):\n",
    "    \"\"\"Create a random classification problem.\"\"\"\n",
    "    A = normalized_data_matrix(m, n, mu)\n",
    "    x0 = sps.rand(n, 1, rho)\n",
    "    x0.data = np.random.randn(x0.nnz)\n",
    "    x0 = x0.toarray().ravel()\n",
    "\n",
    "    b = np.sign(A.dot(x0) + sigma*np.random.randn(m))\n",
    "    return A, b\n",
    "\n",
    "np.random.seed(0)\n",
    "m = 1500\n",
    "n = 50000\n",
    "rho = 0.01\n",
    "mu = 0.1\n",
    "\n",
    "A, b = create_classification(m, n, rho = rho, mu = mu)\n",
    "\n",
    "ratio = float(np.sum(b==1)) / len(b)\n",
    "lambda_max = np.abs((1-ratio)*A[b==1,:].sum(axis=0) +\n",
    "                    ratio*A[b==-1,:].sum(axis=0)).max()\n",
    "lam = 0.5*lambda_max\n",
    "\n",
    "x = cp.Variable(A.shape[1])\n",
    "\n",
    "\n",
    "# Problem construction\n",
    "prob = None\n",
    "opt_val = None\n",
    "\n",
    "def logistic_loss(theta, X, y):\n",
    "    if not all(np.unique(y) == [-1, 1]):\n",
    "        raise ValueError(\"y must have binary labels in {-1,1}\")\n",
    "    return cp.sum_entries(cp.logistic(-sps.diags([y],[0])*X*theta))\n",
    "\n",
    "f = logistic_loss(x, A, b) + lam*cp.norm1(x)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "\n",
    "\n",
    "problemDict = {\n",
    "    \"problemID\": \"logreg_l1_0\",\n",
    "    \"problem\": prob,\n",
    "    \"opt_val\": None\n",
    "}\n",
    "\n",
    "problems = [problemDict]\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    def printResults(problemID = \"\", problem = None, opt_val = None):\n",
    "        print(problemID)\n",
    "        problem.solve()\n",
    "        print(\"\\tstatus: {}\".format(problem.status))\n",
    "        print(\"\\toptimal value: {}\".format(problem.value))\n",
    "        print(\"\\ttrue optimal value: {}\".format(opt_val))\n",
    "    printResults(**problems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
