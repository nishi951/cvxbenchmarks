{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite Push\n",
    "\n",
    "Given a dataset $X_p = \\{x^{+}_1,...,x^{+}_m\\}$ of data vectors classified as \"relevant\" and $X_n = \\{x^{-}_1,...,x^{-}_n\\}$ of data vectors classified as \"irrelevant\", we would like to come up with a linear ranking function $f(x) = w^Tx$ that maximizes the number of relevant vectors in $X_p$ ranked above the highest ranking irrelevant vector. Another way of saying this is that we would like to minimize the maximum number of relevant vectors ranked below any irrelevant vector. In other words, we would like to minimize the function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{1 \\leq j \\leq n} \\sum_{i = 1}^m \\mathbb{1}[f(x^{+}_i) < f(x^{-}_j)]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbb{1}$ is the indicator function that is 1 when its argument is true and false otherwise.\n",
    "\n",
    "Since this function is not convex (the indicator function is discontinuous) we replace the indicator function with its hinge loss convex relaxation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{1 \\leq j \\leq n} \\sum_{i = 1}^m \\max(1 - (f(x^{+}_i) - f(x^{-}_j)), 0)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{1 \\leq j \\leq n} \\sum_{i = 1}^m \\max(1 - w^T(x^{+}_i - x^{-}_j), 0)\n",
    "\\end{equation}\n",
    "\n",
    "This is a pointwise maximum of affine functions of $w$ and is therefore convex in $w$.\n",
    "\n",
    "We add an $\\ell_2$ regularization term to form the final problem:\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\max_{1 \\leq j \\leq n} \\sum_{i = 1}^m \\max(1 - w^T(x^{+}_i - x^{-}_j), 0) + \\lambda \\|w|_2^2 \\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "with variable $w$, regularization parameter $\\lambda$, and constants $x^{+}_i$ for $i = 1,...,m$ and $x^{-}_j$ for $j = 1,...,n$.\n",
    "\n",
    "References: Agarwal, Shivani 2011. \"The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite_push_0\n",
      "\tstatus: optimal\n",
      "\toptimal value: 100.0\n",
      "\ttrue optimal value: None\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# setup\n",
    "\n",
    "problemID = \"infinite_push_0\"\n",
    "prob = None\n",
    "opt_val = None\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "\n",
    "def normalized_data_matrix(m, n, mu):\n",
    "    if mu == 1:\n",
    "        # dense\n",
    "        A = np.random.randn(m, n)\n",
    "        A /= np.sqrt(np.sum(A**2, 0))\n",
    "    else:\n",
    "        # sparse\n",
    "        A = sps.rand(m, n, mu)\n",
    "        A.data = np.random.randn(A.nnz)\n",
    "        N = A.copy()\n",
    "        N.data = N.data**2\n",
    "        A = A*sps.diags([1 / np.sqrt(np.ravel(N.sum(axis=0)))], [0])\n",
    "\n",
    "    return A\n",
    "\n",
    "m = 100\n",
    "n = 200\n",
    "d = 20\n",
    "np.random.seed(0)\n",
    "\n",
    "Xp = normalized_data_matrix(m, d, 1)\n",
    "Xn = normalized_data_matrix(n, d, 1)\n",
    "lam = 1\n",
    "\n",
    "\n",
    "# Problem construction\n",
    "\n",
    "def infinite_push(theta, Xp, Xn):\n",
    "    m, d = Xp.shape\n",
    "    n = Xn.shape[0]\n",
    "    Z = cp.max_elemwise(\n",
    "        1 - (Xp*theta*np.ones((1,n)) - (Xn*theta*np.ones((1,m))).T), 0)\n",
    "    return cp.max_entries(cp.sum_entries(Z, axis=0))\n",
    "\n",
    "theta = cp.Variable(d)\n",
    "f = infinite_push(theta, Xp, Xn) + lam*cp.sum_squares(theta)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "\n",
    "\n",
    "# Problem collection\n",
    "\n",
    "# Single problem collection\n",
    "problemDict = {\n",
    "    \"problemID\" : problemID,\n",
    "    \"problem\"   : prob,\n",
    "    \"opt_val\"   : opt_val\n",
    "}\n",
    "problems = [problemDict]\n",
    "\n",
    "\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    def printResults(problemID = \"\", problem = None, opt_val = None):\n",
    "        print(problemID)\n",
    "        problem.solve()\n",
    "        print(\"\\tstatus: {}\".format(problem.status))\n",
    "        print(\"\\toptimal value: {}\".format(problem.value))\n",
    "        print(\"\\ttrue optimal value: {}\".format(opt_val))\n",
    "    printResults(**problems[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
