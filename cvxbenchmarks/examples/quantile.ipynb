{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Regression\n",
    "\n",
    "Given data $(x_1, y_1),...,(x_m, y_m)$ where the $x_i$'s are vectors of regressors and the $y_i$'s are the dependent variable we are interested in predicting. Quantile regression is a method for estimating the parameters $\\theta$ such that $F^{-1}(x^T\\theta) = \\alpha$, where $F^{-1}$ is the cdf of the distribution $Y|X = x$, and $\\alpha \\in (0, 1)$. In other words, we want $\\theta$ such that \n",
    "\n",
    "\\begin{equation}\n",
    "P(Y \\leq x^T\\theta | X = x) = \\alpha\n",
    "\\end{equation}\n",
    "\n",
    "Note that $\\tau$ is sometimes used instead of $\\alpha$.\n",
    "\n",
    "For example, the method of Least Absolute Deviations (aka $\\ell_1$-norm regression, median regression) finds the parameters $\\theta$ such that $x^T\\theta - y$ is minimized. This, incidentally, is exactly the same as finding the parameters such that $\\hat y = x^T \\theta$ is the median of the distribution $Y | X = x$.\n",
    "\n",
    "To find these parameters $\\theta$, we minimize the **quantile loss**, which is (expressed compactly, noting that there are other equivalent forms for this loss):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_\\alpha(x^T\\theta - y) = (x^T\\theta - y)(\\mathbb{1}_{x^T\\theta - y \\geq 0} - \\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mathbb{1}_{y - x^T\\theta \\geq 0}$ is the indicator function that is $1$ when $y - x^T\\theta \\geq0$ and $0$ otherwise. \n",
    "\n",
    "The quantile loss penalizes overshooting (i.e. $x^T\\theta - y \\geq 0$) with a penalty of $(x^T\\theta - y)(1 - \\alpha)$ and undershooting (i.e. $x^T\\theta - y < 0$) with a penalty of $(x^T\\theta - y)(-\\alpha)$. When $\\alpha$ is large (we have $\\left| \\alpha \\right | \\geq \\left| 1 - \\alpha \\right|$) we are looking for a high quantile (i.e. we want $x^T\\theta$ to give a value that is larger than most $y$ values), so we penalize undershooting more than overshooting. Similarly, when $\\alpha$ is small, we penalize overshooting more than undershooting. When $\\alpha = 1/2$, we penalize undershooting and overshooting by exactly the same amount. See the first reference for a more detailed derivation as to why this might be a suitable loss function.\n",
    "\n",
    "Therefore, for a given $\\alpha$, we would like to solve the problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && (x^T\\theta - y)(\\mathbb{1}_{x^T\\theta - y \\geq 0} - \\alpha) \\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "However, if we are solving this problem for many different values of $\\alpha$, we may wish to impose the further constraint that the quantiles do not contradict each other. We would like to ensure that our estimate of the 90th quantile, for example, is always larger than our estimate of the 89th quantile. In other words, we would like to ensure that given $\\theta_\\alpha$ and $\\theta_\\beta$, corresponding to quantiles $\\alpha$ and $\\beta$ respectively, with $\\alpha < \\beta$, we want to make sure that every $x_i$ gives a larger value when dotted with $\\theta_\\alpha$ than with $\\theta_\\beta$. In other words for a fixed set of $y_i$'s, we expect that assuming these $y_i$'s represent a higher quantile should result in lower values of $x_i^T\\theta$ over the dataset. Therefore, we impose the additional constraint that:\n",
    "\n",
    "\\begin{equation}\n",
    "x_i^T(\\theta_\\beta - \\theta_\\alpha) \\geq 0\n",
    "\\end{equation}\n",
    "\n",
    "This gives us the final problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\sum_{j = 1}^k \\sum_{i = 1}^m (x_i^T\\theta_j - y_i)(\\mathbb{1}_{x_i^T\\theta_j - y_i \\geq 0} - \\alpha_j) \\\\\n",
    "    &\\text{subject to} && x_i^T(\\theta_{\\alpha_l} - \\theta_{\\alpha_m}) >= 0 &&& i = 1,...,m \\\\\n",
    "    &                  &&                                                   &&& 1 \\leq m < l \\leq k\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "with variables $\\theta_j$, $j = 1,..., k$, quantiles $\\alpha_1,...\\alpha_k$ (in increasing order), and labelled data $(x_1, y_1),...,(x_m, y_m)$.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "https://stats.stackexchange.com/questions/251600/quantile-regression-loss-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile_0\n",
      "\tstatus: optimal\n",
      "\toptimal value: 718.7588164416889\n",
      "\ttrue optimal value: None\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# setup\n",
    "\n",
    "problemID = \"quantile_0\"\n",
    "prob = None\n",
    "opt_val = None\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(0)\n",
    "m = 400 # Number of data entries\n",
    "n = 10 # Number of weights\n",
    "k = 100 # Number of quantiles\n",
    "p = 1\n",
    "sigma = 0.1\n",
    "\n",
    "x = np.random.rand(m)*2*np.pi*p\n",
    "y = np.sin(x) + sigma*np.sin(x)*np.random.randn(m)\n",
    "alphas = np.linspace(1./(k+1), 1-1./(k+1), k) # Do a bunch of quantiles at once\n",
    "\n",
    "# RBF (Radial Basis Function) features\n",
    "mu_rbf = np.array([np.linspace(-1, 2*np.pi*p+1, n)])\n",
    "mu_sig = (2*np.pi*p+2)/n\n",
    "X = np.exp(-(mu_rbf.T - x).T**2/(2*mu_sig**2)) # Gaussian\n",
    "# X has dimension m x n\n",
    "\n",
    "Theta = cp.Variable(n,k)\n",
    "\n",
    "\n",
    "# Problem construction\n",
    "\n",
    "def quantile_loss(alphas, Theta, X, y):\n",
    "    m, n = X.shape\n",
    "    k = len(alphas)\n",
    "    Y = np.tile(y.flatten(), (k, 1)).T\n",
    "    A = np.tile(alphas, (m, 1))\n",
    "    Z = X*Theta - Y\n",
    "    return cp.sum_entries(\n",
    "        cp.max_elemwise(\n",
    "            cp.mul_elemwise( -A, Z),\n",
    "            cp.mul_elemwise(1-A, Z)))\n",
    "\n",
    "f = quantile_loss(alphas, Theta, X, y)\n",
    "# C = [X*(Theta[:,:-1] - Theta[:,1:]) >= 0]\n",
    "C = [X*(Theta[:,1:] - Theta[:,:-1]) >= 0]\n",
    "prob = cp.Problem(cp.Minimize(f), C)\n",
    "\n",
    "\n",
    "# Problem collection\n",
    "\n",
    "# Single problem collection\n",
    "problemDict = {\n",
    "    \"problemID\" : problemID,\n",
    "    \"problem\"   : prob,\n",
    "    \"opt_val\"   : opt_val\n",
    "}\n",
    "problems = [problemDict]\n",
    "\n",
    "\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    def printResults(problemID = \"\", problem = None, opt_val = None):\n",
    "        print(problemID)\n",
    "        problem.solve()\n",
    "        print(\"\\tstatus: {}\".format(problem.status))\n",
    "        print(\"\\toptimal value: {}\".format(problem.value))\n",
    "        print(\"\\ttrue optimal value: {}\".format(opt_val))\n",
    "    printResults(**problems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
