{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classification (with L2 Regularization)\n",
    "\n",
    "For the multiclass classification problem, let $s_j$ be the score that a classifier assigns to class $j$, and let $y$ be the true class label. By interpreting the scores of the classifier as a set of unnormalized log probabilities, we can come up with the softmax loss function, which is essentially the KL divergence between the distribution assigning all probability mass to the correct class label and the distribution reported by the classifier. See http://cs231n.github.io/linear-classify/ for an excellent explanation of the softmax loss:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell(s) = - \\log \\frac{e^{s_y}}{\\sum_{j = 1}^n e^{s_j}} = -s_y + \\log \\sum_{j = 1}^n e^{s_j}\n",
    "\\end{equation}\n",
    "\n",
    "We will use the softmax loss to train a linear classifier as follows. Given labelled data $(x_1, y_1), (x_2, y_2),... (x_m, y_m)$, We wish to find a matrix $\\Theta$ and an offset vector $\\beta$ such that the scores given by $s^{(i)} = x_i \\Theta + \\beta$ minimize the loss:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell(X \\Theta) = \\frac{1}{m}\\sum_{i = 1}^m \\left[ - s^{(i)}_{y_i}  + \\log \\sum_{i = 1}^n \\exp(s^{(i)}_j) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "In this particular problem instance, we minimize the sum of the largest $p$ softmax losses, plus an L2 regularization term on the weight matrix $\\Theta$. AS before, let $s^{(i)} = x_i \\Theta + \\beta$ be the vector of scores for the data entry $(x_i, y_i)$. Then let $z$ be the vector such that:\n",
    "\n",
    "\\begin{equation}\n",
    "z_i = -s^{(i)}_{y_i} + \\log \\sum_{j = 1}^n \\exp(s^{(i)}_j)\n",
    "\\end{equation}\n",
    "\n",
    "Our final problem is\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\sum_{l = 1}^p z_{[p]} + \\|\\Theta\\|_2^2 \\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "With variables $\\Theta$ and $\\beta$ and data $(x_1, y_1), (x_2, y_2),... (x_m, y_m)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('status:', 'optimal')\n",
      "('optimal value:', 14.943228062862916)\n",
      "('true optimal value:', None)\n",
      "('status:', 'optimal')\n",
      "('optimal value:', 14.943217551252047)\n",
      "('true optimal value:', None)\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "def normalized_data_matrix(m, n, mu):\n",
    "    if mu == 1:\n",
    "        # dense\n",
    "        A = np.random.randn(m, n)\n",
    "        A /= np.sqrt(np.sum(A**2, 0))\n",
    "    else:\n",
    "        # sparse\n",
    "        A = sps.rand(m, n, mu)\n",
    "        A.data = np.random.randn(A.nnz)\n",
    "        N = A.copy()\n",
    "        N.data = N.data**2\n",
    "        A = A*sps.diags([1 / np.sqrt(np.ravel(N.sum(axis=0)))], [0])\n",
    "\n",
    "    return A\n",
    "\n",
    "np.random.seed(0)\n",
    "k = 20  # Number of classes\n",
    "m = 100 # Number of instances\n",
    "n = 50  # Dimension of each instance\n",
    "p = 5   # p-largest\n",
    "X = normalized_data_matrix(m,n,1) # Randomly generated data\n",
    "Y = np.random.randint(0, k, m) # Randomly generated class scores\n",
    "\n",
    "\n",
    "\n",
    "# Problem construction\n",
    "problems = []\n",
    "opt_vals = []\n",
    "\n",
    "# Problem 1 (Unconstrained)\n",
    "Theta = cp.Variable(n,k)\n",
    "beta = cp.Variable(1,k)\n",
    "obs = cp.vstack([-(X[i]*Theta + beta)[Y[i]] + cp.log_sum_exp(X[i]*Theta + beta) for i in range(m)])\n",
    "problems.append(cp.Problem(cp.Minimize(cp.sum_largest(obs, p) + cp.sum_squares(Theta))))\n",
    "opt_vals.append(None)\n",
    "\n",
    "# Problem 2 (Epigraph form)\n",
    "def one_hot(y, k):\n",
    "    m = len(y)\n",
    "    return sps.coo_matrix((np.ones(m), (np.arange(m), y)), shape=(m, k)).todense()\n",
    "\n",
    "Theta2 = cp.Variable(n,k)\n",
    "beta2 = cp.Variable(1, k)\n",
    "t = cp.Variable(m)\n",
    "texp = cp.Variable(m)\n",
    "f = cp.sum_largest(t+texp, p) + cp.sum_squares(Theta2)\n",
    "C = []\n",
    "C.append(cp.log_sum_exp(X*Theta2 + np.ones((m, 1))*beta2, axis=1) <= texp)\n",
    "Yi = one_hot(Y, k)\n",
    "C.append(t == cp.vstack([-(X[i]*Theta2 + beta2)[Y[i]] for i in range(m)]))\n",
    "problems.append(cp.Problem(cp.Minimize(f), C))\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    for prob in problems:\n",
    "        prob.solve(solver = \"SCS\", eps = 1e-5)\n",
    "        print(\"status:\", prob.status)\n",
    "        print(\"optimal value:\", prob.value)\n",
    "        print(\"true optimal value:\", opt_val)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
