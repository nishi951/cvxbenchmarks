{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fused Lasso\n",
    "\n",
    "The Fused Lasso (Tibshirani, R. and Saunders, M, 2005, \"Sparsity and Smoothness via the fused lasso\") is an extension to the usual Lasso method that also takes the total variation between adjacent parameters into account. Whereas the usual lasso can be written as (with variable $x$):\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\|Ax -b \\|_2^2 + \\lambda \\|x\\|_1 \\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $A \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "If the parameters $x$ have some logical or natural ordering to them, then the fused lasso also encourages sparsity in the first difference of $x$:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\|Ax -b \\|_2^2 + \\lambda_1 \\|x\\|_1 + \\lambda_2 \\sum_{i = 1}^{n-1}\\left|x_{i+1} - x_i\\right|\\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the last term is exactly the total variation operator applied to $x$. In general, $\\lambda_1$ need not be equal to $\\lambda_2$, but we have resused the same lambda here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fused_lasso_0\n",
      "\tstatus: optimal\n",
      "\toptimal value: 74.1042328719\n",
      "\ttrue optimal value: None\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# setup\n",
    "\n",
    "problemID = \"fused_lasso_0\"\n",
    "prob = None\n",
    "opt_val = None\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "m = 1000\n",
    "ni = 10\n",
    "k = 1000\n",
    "rho=0.05\n",
    "sigma=0.05\n",
    "np.random.seed(0)\n",
    "\n",
    "A = np.random.randn(m, ni*k)\n",
    "A /= np.sqrt(np.sum(A**2, 0))\n",
    "\n",
    "x0 = np.zeros(ni*k)\n",
    "for i in range(k):\n",
    "    if np.random.rand() < rho:\n",
    "        x0[i*ni:(i+1)*ni] = np.random.rand()\n",
    "b = A.dot(x0) + sigma*np.random.randn(m)\n",
    "lam = 0.1*sigma*np.sqrt(m*np.log(ni*k))\n",
    "\n",
    "\n",
    "# Problem construction\n",
    "\n",
    "x = cp.Variable(A.shape[1])\n",
    "f = cp.sum_squares(A*x - b) + lam*cp.norm1(x) + lam*cp.tv(x)\n",
    "prob = cp.Problem(cp.Minimize(f))\n",
    "\n",
    "\n",
    "# Problem collection\n",
    "\n",
    "# Single problem collection\n",
    "problemDict = {\n",
    "    \"problemID\" : problemID,\n",
    "    \"problem\"   : prob,\n",
    "    \"opt_val\"   : opt_val\n",
    "}\n",
    "problems = [problemDict]\n",
    "\n",
    "\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    def printResults(problemID = \"\", problem = None, opt_val = None):\n",
    "        print(problemID)\n",
    "        problem.solve()\n",
    "        print(\"\\tstatus: {}\".format(problem.status))\n",
    "        print(\"\\toptimal value: {}\".format(problem.value))\n",
    "        print(\"\\ttrue optimal value: {}\".format(opt_val))\n",
    "    printResults(**problems[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
