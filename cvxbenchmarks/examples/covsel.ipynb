{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Covariance Estimation for Gaussian Random Vectors\n",
    "\n",
    "Reference: Section 7.1.1, Boyd & Vandenberghe \"Convex Optimization\"\n",
    "\n",
    "The problem is to find a sparse estimate $\\Theta$ of an inverse covariance matrix (aka information or concentration matrix) of a Gaussian random vector $X$ with covariance matrix $R$. We are given a bunch of independent empirical measurements $x_1,...,x_m$ and can use them to find an empirical estimate of the covariance matrix $S = \\frac{1}{m}\\sum X_iX_i^T$. The maximum likelihood estimate of the covariance matrix is given by maximizing the log likelihood of the multivariate Gaussian pdf:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(X) = (-mn/2) \\log 2\\pi - (m/2) \\log \\det R - (1/2) \\sum_{k = 1}^m x_k^TR^{-1}x_k \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "= (-mn/2) \\log 2\\pi - (m/2) \\log \\det R - (m/2) \\text{tr}(RS)  \n",
    "\\end{equation}\n",
    "\n",
    "This is not concave in $R$, but it is convex in $\\Theta = R^{-1}$. The ML estimation problem becomes:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && - \\log \\det \\Theta + \\text{tr}(S\\Theta)  \n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "To reward sparse solutions, we add an $\\ell_1$ regularization term on the off-diagonals of the information matrix to get the final problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\lambda\\sum_{i \\neq j} \\left| \\Theta_{ij} \\right| - \\log \\det \\Theta + \\text{tr}(S\\Theta)  \n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "We also require that $\\Theta$ be positive semidefinite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "m = 10\n",
    "n = 20\n",
    "lam = float(0.1)\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "A = sps.rand(n,n, 0.01)\n",
    "A = np.asarray(A.T.dot(A).todense() + 0.1*np.eye(n))\n",
    "L = np.linalg.cholesky(np.linalg.inv(A)) # Sparse\n",
    "X = np.random.randn(m,n).dot(L.T) # Draw m experiments according to the covariance matrix A^-1\n",
    "S = X.T.dot(X)/m # Estimate of covariance matrix\n",
    "W = np.ones((n,n)) - np.eye(n)\n",
    "\n",
    "Theta = cp.Variable(n, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we need to write $\\texttt{Theta = cp.Semidef(n, n)}$? Note the objective function is symmetric across the diagonal of the variable $\\Theta$. Therefore, we expect the solution to be symmetric since given some asymmetric matrix $B$,  we will find that $B^T$ gives the same objective value, and since $C = (B+B^T)/2$ is symmetric, by the convexity of the objective, $C$ gives an objective value no larger than that of $B$.\n",
    "\n",
    "The solution must be positive definite because that is the domain of the log det function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('status:', 'optimal')\n",
      "('optimal value:', 32.21413536302113)\n",
      "('true optimal value:', None)\n"
     ]
    }
   ],
   "source": [
    "# Problem construction\n",
    "prob = None\n",
    "opt_val = None\n",
    "\n",
    "prob = cp.Problem(cp.Minimize(\n",
    "        lam*cp.norm1(cp.mul_elemwise(W,Theta)) +\n",
    "        cp.sum_entries(cp.mul_elemwise(S,Theta)) -\n",
    "        cp.log_det(Theta)))\n",
    "\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    prob.solve()\n",
    "    print(\"status:\", prob.status)\n",
    "    print(\"optimal value:\", prob.value)\n",
    "    print(\"true optimal value:\", opt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
