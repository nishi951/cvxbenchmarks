{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust SVM\n",
    "\n",
    "The robust SVM problem is an extention to the original SVM problem that takes into account measurement uncertainty or variation in the location of the data points that we wish to separate. A typical ($\\ell_2$-regularized) SVM might look like:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\lambda\\|x\\|^2 + \\sum_{i = 1}^m \\max(1 - b_i a_i^T x, 0) \\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $x$ is the variable, $a_1,...,a_m$ are the (zero-centered) data vectors, and $b_1,...,b_m$ are their respective labels in $\\{-1, +1\\}$.\n",
    "\n",
    "To add robustness to our model, we would like to find the value of $x$ that is still optimal when the data values $a_i$ have all been perturbed in some fashion.. For example, assume the data values $a_i$ are perturbed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "a_i \\to a_i + P\\delta_i\n",
    "\\end{equation}\n",
    "\n",
    "where $P$ is a (known) perturbation matrix and $\\delta = (\\delta_1,...,\\delta_m)$ is an (unknown) collection of vectors constrained to be in some set $\\mathcal{D}$.\n",
    "\n",
    "Taking $\\mathcal{D}$ to be\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{D} = \\{ \\|\\delta_i\\|_\\infty \\leq 1 \\;\\mid\\; i = 1,...,m \\}\n",
    "\\end{equation}\n",
    "\n",
    "we produce the modified objective:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\delta \\in \\mathcal{D}}\\left[ \\lambda\\|x\\|^2 + \\sum_{i = 1}^m \\max(1 - b_i (a_i + P\\delta)^T x, 0) \\right]\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "= \\max_{\\delta \\in \\mathcal{D}}\\left[ \\lambda\\|x\\|^2 + \\sum_{i = 1}^m \\max(1 - b_i a_i^Tx + \\delta^T P^Tx, 0) \\right]\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "=\\lambda\\|x\\|^2 + \\sum_{i = 1}^m \\max(1 - b_i a_i^Tx + \\|P^T x\\|_1, 0)\n",
    "\\end{equation}\n",
    "\n",
    "The above derivation uses the definition of dual norm (See Convex Optimization, Boyd and Vandenberghe, Appendix A1.6) and the fact that the $\\ell_\\infty$ and $\\ell_1$ norms are dual (also see HÃ¶lder's inequality). Because we are maximizing over $\\mathcal{D}$, this is the **worst-case** robust SVM.\n",
    "\n",
    "Putting all this together, we form the new robust, regularized SVM problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\lambda\\|x\\|^2 + \\sum_{i = 1}^m \\max(1 - b_i a_i^Tx + \\|P^T x\\|_1, 0) \\\\\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "with variable $x$, data vectors $a_i$ with corresponding labels $b_i$, and perturbation matrix $P$.\n",
    "\n",
    "We can also write this problem in an epigraph form:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    &\\text{minimize} && \\lambda\\|x\\|^2 + \\sum_{i = 1}^m \\max(1 - b_i a_i^Tx + t, 0) \\\\\n",
    "    &\\text{subject to} && \\|P^T x\\|_1 \\leq t \n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "https://people.eecs.berkeley.edu/~elghaoui/Talks/talkNeyman2008.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust_svm_0\n",
      "\tstatus: optimal\n",
      "\toptimal value: 116.65257077276507\n",
      "\ttrue optimal value: None\n",
      "robust_svm_0_epigraph\n",
      "\tstatus: optimal\n",
      "\toptimal value: 116.65257125050829\n",
      "\ttrue optimal value: None\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# setup\n",
    "\n",
    "problemID = \"robust_svm_0\"\n",
    "prob = None\n",
    "opt_val = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variable declarations\n",
    "\n",
    "import scipy.sparse as sps\n",
    "import scipy.linalg as la\n",
    "\n",
    "def normalized_data_matrix(m, n, mu):\n",
    "    if mu == 1:\n",
    "        # dense\n",
    "        A = np.random.randn(m, n)\n",
    "        A /= np.sqrt(np.sum(A**2, 0))\n",
    "    else:\n",
    "        # sparse\n",
    "        A = sps.rand(m, n, mu)\n",
    "        A.data = np.random.randn(A.nnz)\n",
    "        N = A.copy()\n",
    "        N.data = N.data**2\n",
    "        A = A*sps.diags([1 / np.sqrt(np.ravel(N.sum(axis=0)))], [0])\n",
    "\n",
    "    return A\n",
    "\n",
    "np.random.seed(0)\n",
    "m = 200\n",
    "n = 60\n",
    "mu = 1\n",
    "rho = 1\n",
    "sigma = 0.1\n",
    "\n",
    "A = normalized_data_matrix(m, n, mu)\n",
    "x0 = sps.rand(n, 1, rho)\n",
    "x0.data = np.random.randn(x0.nnz)\n",
    "x0 = x0.toarray().ravel()\n",
    "\n",
    "# Move positive entries more positive, negative entries more negative.\n",
    "# (w.r.t. inner product with x_0)\n",
    "b = np.sign(A.dot(x0) + sigma*np.random.randn(m))\n",
    "A[b>0,:] += 0.7*np.tile([x0], (np.sum(b>0),1))\n",
    "A[b<0,:] -= 0.7*np.tile([x0], (np.sum(b<0),1))\n",
    "\n",
    "# Noise Perturbation matrix\n",
    "P = la.block_diag(np.random.randn(n-1,n-1), 0)\n",
    "lam = 1\n",
    "\n",
    "# Problem 1: Unconstrained\n",
    "x1 = cp.Variable(n)\n",
    "z1 = 1 - sps.diags([b],[0])*A*x1 + cp.norm1(P.T*x1) \n",
    "f = lam*cp.sum_squares(x1) + cp.sum_entries(cp.max_elemwise(z1, 0))\n",
    "prob1 = cp.Problem(cp.Minimize(f))\n",
    "\n",
    "# Problem 2: Epigraph formulation\n",
    "x2 = cp.Variable(A.shape[1])\n",
    "t = cp.Variable(1)\n",
    "z2 = 1 - sps.diags([b],[0])*A*x2 + t\n",
    "f = lam*cp.sum_squares(x2) + cp.sum_entries(cp.max_elemwise(z2, 0))\n",
    "C = [cp.norm1(P.T*x2) <= t]\n",
    "prob2 = cp.Problem(cp.Minimize(f), C)\n",
    "\n",
    "\n",
    "# Problem collection\n",
    "\n",
    "# Single problem collection\n",
    "problem1Dict = {\n",
    "    \"problemID\" : problemID,\n",
    "    \"problem\"   : prob1,\n",
    "    \"opt_val\"   : opt_val\n",
    "}\n",
    "problem2Dict = {\n",
    "    \"problemID\" : problemID+\"_epigraph\",\n",
    "    \"problem\"   : prob2,\n",
    "    \"opt_val\"   : opt_val\n",
    "}\n",
    "\n",
    "problems = [problem1Dict, problem2Dict]\n",
    "\n",
    "# For debugging individual problems:\n",
    "if __name__ == \"__main__\":\n",
    "    def printResults(problemID = \"\", problem = None, opt_val = None):\n",
    "        print(problemID)\n",
    "        problem.solve()\n",
    "        print(\"\\tstatus: {}\".format(problem.status))\n",
    "        print(\"\\toptimal value: {}\".format(problem.value))\n",
    "        print(\"\\ttrue optimal value: {}\".format(opt_val))\n",
    "    printResults(**problems[0])\n",
    "    printResults(**problems[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
